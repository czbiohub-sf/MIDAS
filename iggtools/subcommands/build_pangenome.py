import os
import sys
from collections import defaultdict
from multiprocessing import Semaphore
import Bio.SeqIO
from iggtools.common.argparser import add_subcommand
from iggtools.common.utils import tsprint, InputStream, OutputStream, parse_table, retry, command, split, multiprocessing_map, multithreading_hashmap, multithreading_map, num_vcpu, transpose, find_files, sorted_dict, upload, upload_star, flatten
from iggtools.params import outputs


CLUSTERING_PERCENTS = [99, 95, 90, 85, 80, 75]
CLUSTERING_PERCENTS = sorted(CLUSTERING_PERCENTS, reverse=True)


# Up to this many concurrent species builds.
CONCURRENT_SPECIES_BUILDS = Semaphore(3)


def pangenome_file(representative_id, component):
    # s3://microbiome-igg/2.0/pangenomes/GUT_GENOMEDDDDDD/{genes.ffn, centroids.ffn, gene_info.txt}
    return f"{outputs.pangenomes}/{representative_id}/{component}"


def annotations_file(genome_id, component_extension):
    # s3://microbiome-igg/2.0/prodigal/GUT_GENOMEDDDDDD.{fna, faa, gff, log}
    return f"{outputs.annotations}/{genome_id}.{component_extension}"


def read_toc(genomes_tsv, deep_sort=False):
    # Read in the table of contents.
    # We will centralize and improve this soon.
    species = defaultdict(dict)
    representatives = {}
    with InputStream(genomes_tsv) as table_of_contents:
        for row in parse_table(table_of_contents, ["genome", "species", "representative", "genome_is_representative"]):
            genome_id, species_id, representative_id, _ = row
            species[species_id][genome_id] = row
            representatives[species_id] = representative_id
    if deep_sort:
        for sid in species.keys():
            species[sid] = sorted_dict(species[sid])
        species = sorted_dict(species)
    return species, representatives


# 1. Occasional failures in aws s3 cp require a retry.
# 2. In future, for really large numbers of genomes, we may prefer instead a separate wave of retries for the first-attempt failures.
# 3. The Bio.SeqIO.parse() code is CPU-bound and thus it's best to run this function in a separate process for every genome.
@retry
def clean_genes(genome_id):
    input_annotations = annotations_file(genome_id, "fna.lz4")
    output_genes = f"{genome_id}.genes.fna"
    output_info = f"{genome_id}.genes.len"

    with open(output_genes, 'w') as o_genes, \
         open(output_info, 'w') as o_info, \
         InputStream(input_annotations, check_path=False) as genes:
        for rec in Bio.SeqIO.parse(genes, 'fasta'):
            gene_id = rec.id
            gene_seq = str(rec.seq).upper()
            gene_len = len(gene_seq)
            if gene_len == 0 or gene_id == '' or gene_id == '|':
                # Documentation for why we ignore these gene_ids should be added to
                # https://github.com/czbiohub/iggtools/wiki#pan-genomes
                # Also, we should probably count these and report stats.
                pass
            else:
                o_genes.write(f">{gene_id}\n{gene_seq}\n")
                o_info.write(f"{gene_id}\t{genome_id}\t{gene_len}\n")

    return output_genes, output_info


def vsearch(percent_id, genes, num_threads=num_vcpu):
    centroids = f"centroids.{percent_id}.ffn"
    uclust = f"uclust.{percent_id}.txt"
    # log = f"uclust.{percent_id}.log"
    if find_files(centroids) and find_files(uclust):
        tsprint(f"Found vsearch results at percent identity {percent_id} from prior run.")
    else:
        command(f"vsearch --quiet --cluster_fast {genes} --id {percent_id/100.0} --threads {num_threads} --centroids {centroids} --uc {uclust}")
    return centroids, uclust #, log


def parse_uclust(uclust_file, select_columns):
    # The uclust TSV file does not contain a header line.  So, we have to hardcode the schema here.  Then select specified columns.
    all_uclust_columns = ['type', 'cluster_id', 'size', 'pid', 'strand', 'skip1', 'skip2', 'skip3', 'gene_id', 'centroid_id']
    with InputStream(uclust_file) as ucf:
        for r in parse_table(ucf, select_columns, all_uclust_columns):
            yield r


def xref(cluster_files, gene_info_file):
    """
    Produce the gene_info.txt file as documented in https://github.com/czbiohub/iggtools/wiki#pan-genomes
    """
    # Let centroids[gene][percent_id] be the centroid of the percent_id cluster contianing gene.
    #
    # The centroids are themselves genes, and their ids, as all gene_ids, are strings
    # generated by the annotation tool prodigal.
    centroid_info = defaultdict(dict)
    for percent_id, (_, uclust_file) in cluster_files.items():
        for r_type, r_gene, r_centroid in parse_uclust(uclust_file, ['type', 'gene_id', 'centroid_id']):
            if r_type == 'S':
                # r itself is the centroid of its cluster
                centroid_info[r_gene][percent_id] = r_gene
            elif r_type == 'H':
                # r is not itself a centroid
                centroid_info[r_gene][percent_id] = r_centroid
            else:
                # ignore all other r types
                pass

    # At this point we have the max_percent_id centroid for any gene gc, but we lack
    # coarser clustering assignments for many genes -- we only have those for genes
    # that are themelves centroids of max_percent_id clusters.
    #
    # We can infer the remaining cluster assignments for all genes by transitivity.
    # For any gene gc, look up the clusters containing gc's innermost centroid,
    # gc[max_percent_id].  Those clusters also contain gc.
    percents = cluster_files.keys()
    max_percent_id = max(percents)
    for gc in centroid_info.values():
        gc_recluster = centroid_info[gc[max_percent_id]]
        for percent_id in percents:
            gc[percent_id] = gc_recluster[percent_id]

    with OutputStream(gene_info_file) as gene_info:
        header = ['gene_id'] + [f"centroid_{pid}" for pid in percents]
        gene_info.write('\t'.join(header) + '\n')
        genes = centroid_info.keys()
        for gene_id in sorted(genes):
            gene_info.write(gene_id)
            for centroid in centroid_info[gene_id].values():
                gene_info.write('\t')
                gene_info.write(centroid)
            gene_info.write('\n')


def build_pangenome(args):
    if args.slave_toc:
        build_pangenome_slave(args)
    else:
        build_pangenome_master(args)


@retry
def find_files_with_retry(f):
    return find_files(f)


def decode_species_arg(args, species):
    selected_species = set()
    try:
        if args.species.upper() == "ALL":
            selected_species = set(species)
        else:
            for s in args.species.split(","):
                if ":" not in s:
                    assert str(int(s)) == s, f"Species id is not an integer: {s}"
                    selected_species.add(s)
                else:
                    i, n = s.split(":")
                    i = int(i)
                    n = int(n)
                    assert 0 <= i < n, f"Species class and modulus make no sense: {i}, {n}"
                    for sid in species:
                        if int(sid) % n == i:
                            selected_species.add(sid)
    except:
        tsprint(f"ERROR:  Species argument is not a list of species ids or slices: {s}")
        raise
    return sorted(selected_species)


def build_pangenome_master(args):

    # Fetch table of contents from s3.
    # This will be read separately by each species build subcommand, so we make a local copy.
    local_toc = os.path.basename(outputs.genomes)
    command(f"rm -f {local_toc}")
    command(f"aws s3 cp {outputs.genomes} {local_toc}")

    species, representatives = read_toc(local_toc)

    def species_work(species_id):
        assert species_id in species, f"Species {species_id} is not in the database."
        representative_id = representatives[species_id]
        species_genomes = species[species_id]

        def destpath(src):
            return pangenome_file(representative_id, src + ".lz4")

        # The species build will upload this file last, after everything else is successfully uploaded.
        # Therefore, if this file exists in s3, there is no need to redo the species build.
        dest_file = destpath("gene_info.txt")
        msg = f"Building pangenome for species {species_id} with representative genome {representative_id} and {len(species_genomes)} total genomes."
        if find_files_with_retry(dest_file):
            if not args.force:
                tsprint(f"Destination {dest_file} already exists.  Specify --force to overwrite.")
                return
            msg = msg.replace("Building", "Rebuilding")

        with CONCURRENT_SPECIES_BUILDS:
            tsprint(msg)
            if not args.debug:
                command(f"rm -rf {species_id}")
            if not os.path.isdir(str(species_id)):
                command(f"mkdir {species_id}")
            try:
                # Recurisve call via subcommand.  Use subdir, redirect logs.
                myroot = os.path.dirname(os.path.dirname(sys.argv[0]))
                command(f"cd {species_id}; PYTHONPATH={myroot} {sys.executable} -m iggtools build_pangenome -s {species_id} --slave_mode --slave_toc {os.path.abspath(local_toc)} {'--debug' if args.debug else ''} > pangenome_build.log 2>&1")
            finally:
                if not os.path.isfile(f"{species_id}/pangenome_build.log"):
                    command(f"echo 'Failed to even create log file.' > {species_id}/pangenome_build.log")
                try:
                    upload(f"{species_id}/pangenome_build.log", destpath("pangenome_build.log"))
                except:
                    pass
                if not args.debug:
                    command(f"rm -rf {species_id}")

    # Check for destination presence in s3 with up to 10-way concurrency.
    # If destination is absent, commence build with up to 3-way concurrency as constrained by CONCURRENT_SPECIES_BUILDS.
    species_id_list = decode_species_arg(args, species)
    multithreading_map(species_work, species_id_list, num_threads=10)


def build_pangenome_slave(args):
    """
    Input spec:  https://github.com/czbiohub/iggtools/wiki#gene-annotations
    Output spec: https://github.com/czbiohub/iggtools/wiki#pan-genomes
    """

    violation = "Please do not call build_pangenome_slave directly.  Violation"
    assert args.slave_mode, f"{violation}:  Missing --slave_mode arg."
    assert os.path.isfile(args.slave_toc), f"{violation}: File does not exist: {args.slave_toc}"
    assert os.path.basename(os.getcwd()) == args.species, f"{violation}: {os.path.basename(os.getcwd())} != {args.species}"

    species, representatives = read_toc(args.slave_toc)
    species_id = args.species

    assert species_id in species, f"{violation}: Species {species_id} is not in the database."

    species_genomes = species[species_id]
    representative_id = representatives[species_id]
    species_genomes_ids = species_genomes.keys()

    def destpath(src):
        return pangenome_file(representative_id, src + ".lz4")

    command(f"aws s3 rm --recursive {pangenome_file(representative_id, '')}")

    cleaned = multiprocessing_map(clean_genes, species_genomes_ids)

    command("rm -f genes.ffn genes.len")

    for temp_files in split(cleaned, 20):  # keep "cat" commands short
        fna_files, len_files = transpose(temp_files)
        command("cat " + " ".join(fna_files) + " >> genes.ffn")
        command("cat " + " ".join(len_files) + " >> genes.len")

    # The initial clustering to max_percent takes longest.
    max_percent, lower_percents = CLUSTERING_PERCENTS[0], CLUSTERING_PERCENTS[1:]
    cluster_files = {max_percent: vsearch(max_percent, "genes.ffn")}

    # Reclustering of the max_percent centroids is usually quick, and can proceed in prallel.
    recluster = lambda percent_id: vsearch(percent_id, cluster_files[max_percent][0])
    cluster_files.update(multithreading_hashmap(recluster, lower_percents))

    xref(cluster_files, "gene_info.txt")

    # Create list of (source, dest) pairs for uploading.
    # Note that centroids.{max_percent}.ffn is uploaded to 2 different destinations.
    upload_tasks = [
        ("genes.ffn", destpath("genes.ffn")),
        ("genes.len", destpath("genes.len")),
        (f"centroids.{max_percent}.ffn", destpath("centroids.ffn"))  # no percent in dest, per spec
    ]
    for src in flatten(cluster_files.values()):
        upload_tasks.append((src, destpath("temp/" + src)))

    # Upload in parallel.
    multithreading_map(upload_star, upload_tasks)

    # Leave this upload for last, so the presence of this file in s3 would indicate the entire species build has succeeded.
    upload("gene_info.txt", destpath("gene_info.txt"))


def register_args(main_func):
    subparser = add_subcommand('build_pangenome', main_func, help='build pangenome for given species')
    subparser.add_argument('-s',
                           '--species',
                           dest='species',
                           required=False,
                           help="species[,species...] whose pangenome(s) to build;  alternatively, species slice in format idx:modulus, e.g. 1:30, meaning build species whose ids are 1 mod 30; or, the special keyword 'all' meaning all species")
    subparser.add_argument('--slave_toc',
                           dest='slave_toc',
                           required=False,
                           help="reserved to pass table of contents from master to slave")
    return main_func


@register_args
def main(args):
    tsprint(f"Executing iggtools subcommand {args.subcommand} with args {vars(args)}.")
    build_pangenome(args)
