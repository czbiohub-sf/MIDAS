import os
import sys
import traceback
from collections import defaultdict
import Bio.SeqIO
from iggtools.common.argparser import add_subcommand
from iggtools.common.utils import tsprint, InputStream, OutputStream, parse_table, retry, command, split, multiprocessing_map, multithreading_hashmap, multithreading_map, num_vcpu, transpose, find_files, sorted_dict
from iggtools.params import outputs


CLUSTERING_PERCENTS = [99, 95, 90, 85, 80, 75]
CLUSTERING_PERCENTS = sorted(CLUSTERING_PERCENTS, reverse=True)


def pangenome_file(representative_id, component):
    # s3://microbiome-igg/2.0/pangenomes/GUT_GENOMEDDDDDD/{genes.ffn, centroids.ffn, gene_info.txt}
    return f"{outputs.pangenomes}/{representative_id}/{component}"


def annotations_file(genome_id, component_extension):
    # s3://microbiome-igg/2.0/prodigal/GUT_GENOMEDDDDDD.{fna, faa, gff, log}
    return f"{outputs.annotations}/{genome_id}.{component_extension}"


def read_toc(genomes_tsv, deep_sort=False):
    # Read in the table of contents.
    # We will centralize and improve this soon.
    species = defaultdict(dict)
    representatives = {}
    with InputStream(genomes_tsv) as table_of_contents:
        for row in parse_table(table_of_contents, ["genome", "species", "representative", "genome_is_representative"]):
            genome_id, species_id, representative_id, _ = row
            species[species_id][genome_id] = row
            representatives[species_id] = representative_id
    if deep_sort:
        for sid in species.keys():
            species[sid] = sorted_dict(species[sid])
        species = sorted_dict(species)
    return species, representatives


# 1. Occasional failures in aws s3 cp require a retry.
# 2. In future, for really large numbers of genomes, we may prefer instead a separate wave of retries for the first-attempt failures.
# 3. The Bio.SeqIO.parse() code is CPU-bound and thus it's best to run this function in a separate process for every genome.
@retry
def clean_genes(genome_id):
    input_annotations = annotations_file(genome_id, "fna.lz4")
    output_genes = f"{genome_id}.genes.fna"
    output_info = f"{genome_id}.genes.len"

    with open(output_genes, 'w') as o_genes, \
         open(output_info, 'w') as o_info, \
         InputStream(input_annotations, check_path=False) as genes:
        for rec in Bio.SeqIO.parse(genes, 'fasta'):
            gene_id = rec.id
            gene_seq = str(rec.seq).upper()
            gene_len = len(gene_seq)
            if gene_len == 0 or gene_id == '' or gene_id == '|':
                # Documentation for why we ignore these gene_ids should be added to
                # https://github.com/czbiohub/iggtools/wiki#pan-genomes
                # Also, we should probably count these and report stats.
                pass
            else:
                o_genes.write(f">{gene_id}\n{gene_seq}\n")
                o_info.write(f"{gene_id}\t{genome_id}\t{gene_len}\n")

    return output_genes, output_info


def vsearch(percent_id, genes, num_threads=num_vcpu):
    centroids = f"centroids.{percent_id}.ffn"
    uclust = f"uclust.{percent_id}.txt"
    # log = f"uclust.{percent_id}.log"
    if find_files(centroids) and find_files(uclust):
        tsprint(f"Found vsearch results at percent identity {percent_id} from prior run.")
    else:
        command(f"vsearch --quiet --cluster_fast {genes} --id {percent_id/100.0} --threads {num_threads} --centroids {centroids} --uc {uclust}")
    return centroids, uclust #, log


def parse_uclust(uclust_file, select_columns):
    # The uclust TSV file does not contain a header line.  So, we have to hardcode the schema here.  Then select specified columns.
    all_uclust_columns = ['type', 'cluster_id', 'size', 'pid', 'strand', 'skip1', 'skip2', 'skip3', 'gene_id', 'centroid_id']
    with InputStream(uclust_file) as ucf:
        for r in parse_table(ucf, select_columns, all_uclust_columns):
            yield r


def xref(cluster_files, gene_info_file):
    """
    Produce the gene_info.txt file as documented in https://github.com/czbiohub/iggtools/wiki#pan-genomes
    """
    # Let centroids[gene][percent_id] be the centroid of the percent_id cluster contianing gene.
    #
    # The centroids are themselves genes, and their ids, as all gene_ids, are strings
    # generated by the annotation tool prodigal.
    centroid_info = defaultdict(dict)
    for percent_id, (_, uclust_file) in cluster_files.items():
        for r_type, r_gene, r_centroid in parse_uclust(uclust_file, ['type', 'gene_id', 'centroid_id']):
            if r_type == 'S':
                # r itself is the centroid of its cluster
                centroid_info[r_gene][percent_id] = r_gene
            elif r_type == 'H':
                # r is not itself a centroid
                centroid_info[r_gene][percent_id] = r_centroid
            else:
                # ignore all other r types
                pass

    # At this point we have the max_percent_id centroid for any gene gc, but we lack
    # coarser clustering assignments for many genes -- we only have those for genes
    # that are themelves centroids of max_percent_id clusters.
    #
    # We can infer the remaining cluster assignments for all genes by transitivity.
    # For any gene gc, look up the clusters containing gc's innermost centroid,
    # gc[max_percent_id].  Those clusters also contain gc.
    percents = cluster_files.keys()
    max_percent_id = max(percents)
    for gc in centroid_info.values():
        gc_recluster = centroid_info[gc[max_percent_id]]
        for percent_id in percents:
            gc[percent_id] = gc_recluster[percent_id]

    with OutputStream(gene_info_file) as gene_info:
        header = ['gene_id'] + [f"centroid_{pid}" for pid in percents]
        gene_info.write('\t'.join(header) + '\n')
        genes = centroid_info.keys()
        for gene_id in sorted(genes):
            gene_info.write(gene_id)
            for centroid in centroid_info[gene_id].values():
                gene_info.write('\t')
                gene_info.write(centroid)
            gene_info.write('\n')


@retry
def upload(src, dst):
    command(f"set -o pipefail; lz4 -c {src} | aws s3 cp --only-show-errors - {dst}")


def upload_star(srcdst):
    src, dst = srcdst
    return upload(src, dst)


def build_pangenome(args):

    if args.toc:
        build_pangenome_from_subsubcommand(args)
    else:
        build_pangenome_driver(args)


@retry
def my_find_files(f):
    return find_files(f)


def build_pangenome_driver(args):

    local_toc = os.path.basename(outputs.genomes)

    command(f"rm -f {local_toc}")

    command(f"aws s3 cp {outputs.genomes} {local_toc}")

    species, representatives = read_toc(local_toc)

    pretasks = []
    dests = []
    for species_id in args.species.split(","):
        try:
            assert species_id in species, f"Species {species_id} is not in the database."
            representative_id = representatives[species_id]
            species_genomes = species[species_id]
            dest_file = pangenome_file(representative_id, "gene_info.txt.lz4")
            dests.append(dest_file)
            pretasks.append([species_id, representative_id, len(species_genomes), local_toc, args.debug])
        except:
            tsprint(traceback.format_exc())

    dests_exist = multithreading_map(my_find_files, dests, 10)

    tasks = []
    for i in range(len(pretasks)):
        pt = pretasks[i]
        msg = f"Building pangenome for species {pt[0]} with representative genome {pt[1]} and {pt[2]} total genomes."
        if dests_exist[i]:
            if not args.force:
                tsprint(f"Destination {dests[i]} already exists.  Specify --force to overwrite.")
                continue
            msg = msg.replace("Building", "Rebuilding")
        pt.append(msg)
        tasks.append(pt)

    multithreading_map(build_pangenome_single_species, tasks, num_threads=3)


def build_pangenome_single_species(myargs):

    species_id, representative_id, _len_species_genomes, local_toc, args_debug, msg = myargs

    tsprint(msg)

    def destpath(src):
        return pangenome_file(representative_id, src + ".lz4")

    if not args_debug:
        command(f"rm -rf {species_id}")
    if not os.path.isdir(str(species_id)):
        command(f"mkdir {species_id}")

    try:
        # Make essentially a recurisve call via subcommand, redirecting logs.
        myroot = os.path.dirname(os.path.dirname(sys.argv[0]))
        command(f"cd {species_id}; PYTHONPATH={myroot} {sys.executable} -m iggtools build_pangenome -s {species_id} --toc {os.path.abspath(local_toc)} > pangenome_build.log 2>&1")
    finally:
        try:
            upload(f"{species_id}/pangenome_build.log", destpath("pangenome_build.log"))
        except:
            pass
        if not args_debug:
            command(f"rm -rf {species_id}")


def build_pangenome_from_subsubcommand(args):
    """
    Input spec:  https://github.com/czbiohub/iggtools/wiki#gene-annotations
    Output spec: https://github.com/czbiohub/iggtools/wiki#pan-genomes
    """

    assert os.path.isfile(args.toc), f"Please do not call this directly.  Violation: File does not exist: {args.toc}"
    assert os.path.basename(os.getcwd()) == args.species, f"Please do not call this directly.  Violation: {os.path.basename(os.getcwd())} != {args.species}"

    species, representatives = read_toc(args.toc)

    species_id = args.species
    assert species_id in species, f"Species {species_id} is not in the database."

    species_genomes = species[species_id]
    representative_id = representatives[species_id]
    species_genomes_ids = species_genomes.keys()
    tsprint(f"There are {len(species_genomes)} genomes for species {species_id} with representative genome {representative_id}.")

    def destpath(src):
        return pangenome_file(representative_id, src + ".lz4")

    command(f"aws s3 rm --recursive {pangenome_file(representative_id, '')}")

    cleaned = multiprocessing_map(clean_genes, species_genomes_ids)

    command("rm -f genes.ffn")
    command("rm -f genes.len")

    for temp_files in split(cleaned, 20):  # keep "cat" commands short
        fna_files, len_files = transpose(temp_files)
        command("cat " + " ".join(fna_files) + " >> genes.ffn")
        command("cat " + " ".join(len_files) + " >> genes.len")

    max_percent, lower_percents = CLUSTERING_PERCENTS[0], CLUSTERING_PERCENTS[1:]
    cluster_files = {max_percent: vsearch(max_percent, "genes.ffn")}
    recluster = lambda percent_id: vsearch(percent_id, cluster_files[max_percent][0])
    cluster_files.update(multithreading_hashmap(recluster, lower_percents))

    xref(cluster_files, "gene_info.txt")

    # Create list of (source, dest) pairs for uploading.
    # Note one of the sources is copied to 2 different destinations.
    upload_tasks = []
    for src in ["gene_info.txt", "genes.ffn", "genes.len"]:
        upload_tasks.append((src, destpath(src)))
    src = f"centroids.{max_percent}.ffn"
    dst = destpath("centroids.ffn")  # no percent in this
    upload_tasks.append((src, dst))
    # Stash cluster_files under pangenome_dir/temp/
    # This includes an extra copy of centroids.{max_percent}.ffn
    for files in cluster_files.values():
        # files is a pair of (centroid, uclust) files
        for src in files:
            upload_tasks.append((src, destpath("temp/" + src)))

    multithreading_map(upload_star, upload_tasks)


def register_args(main_func):
    subparser = add_subcommand('build_pangenome', main_func, help='build pangenome for given species')
    subparser.add_argument('-s',
                           '--species',
                           dest='species',
                           required=True,
                           help="species whose pangenome to build")
    subparser.add_argument('-9',
                           '--toc',
                           dest='toc',
                           required=False,
                           help="reserved")
    return main_func


@register_args
def main(args):
    tsprint(f"Executing iggtools subcommand {args.subcommand} with args {vars(args)}.")
    build_pangenome(args)
